{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be522a85-71a1-4918-a711-f493aefb26ad",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "996cfee3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#%matplotlib notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f109164",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Computational packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import tqdm\n",
    "\n",
    "# General packages\n",
    "import datetime\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# custom packages\n",
    "import ratsimulator\n",
    "from ratsimulator.Environment import Rectangle\n",
    "import spatial_maps as sm # CINPLA spatial maps\n",
    "\n",
    "# avoid adding multiple relave paths to sys.path\n",
    "sys.path.append(\"../src\") if \"../src\" not in sys.path else None \n",
    "from PlaceCells import PlaceCells\n",
    "from Models import SorscherRNN\n",
    "from Experiment import Experiment\n",
    "from methods import *\n",
    "from datahandling import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fb2b7a-33d6-499f-b899-7f249416b36d",
   "metadata": {},
   "source": [
    "### Setup Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57ff00f3-81cd-4031-8e86-28d9955f6572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment <gg-3CL> is NEW. Loading DEFAULT experiment settings!\n",
      "Singular matrix\n",
      "Singular matrix\n",
      "Default <params>, <environments>, <agents> and <pc_ensembles> can now be changed. Finish setup by calling setup()\n",
      "Singular matrix\n",
      "Singular matrix\n",
      "Singular matrix\n",
      "Singular matrix\n",
      "Creating directories\n",
      "Saving experiment details\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_path = \"/mnt/WD12TB-HDD\"\n",
    "experiment = Experiment(name = 'gg-3CL', base_path = base_path)\n",
    "if experiment.is_new_experiment:\n",
    "    experiment.params['nepochs'] = 3000\n",
    "    experiment.params['sampler'] = \"CLSampler\"\n",
    "    for i in range(1,3):\n",
    "        environments, agents, pc_ensembles = experiment.get_default_ecology(seed = i)\n",
    "        experiment.environments += environments\n",
    "        experiment.agents += agents\n",
    "        experiment.pc_ensembles += pc_ensembles\n",
    "\n",
    "experiment.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "530b5b7b-f788-45d5-a318-61e2cab433a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device=device(type='cuda')\n"
     ]
    }
   ],
   "source": [
    "# detach experiment specifics\n",
    "params = experiment.params\n",
    "environments = experiment.environments\n",
    "agents = experiment.agents\n",
    "pc_ensembles = experiment.pc_ensembles\n",
    "paths = experiment.paths\n",
    "\n",
    "num_workers = 16\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"{device=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626330b4-f133-4ac8-bc0f-2c75b3ae70dd",
   "metadata": {},
   "source": [
    "### Initialise objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cff29f0-2cb5-4eb8-9ece-72a72fb91141",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model = SorscherRNN(\n",
      "  (init_position_encoder): Linear(in_features=512, out_features=4096, bias=False)\n",
      "  (RNN): RNN(2, 4096, bias=False, batch_first=True)\n",
      "  (decoder): Linear(in_features=4096, out_features=512, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialise data loading\n",
    "num_samples = params['nsteps'] * params['batch_size'] # * params['nepochs']\n",
    "dataset = Dataset(agents = agents, pc_ensembles = pc_ensembles, num_samples = num_samples, **params)\n",
    "datasampler = eval(params['sampler'])(num_environments = len(environments), num_samples = num_samples, \\\n",
    "                                      num_epochs = params['nepochs'])\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=params['batch_size'], sampler = datasampler, num_workers=num_workers)\n",
    "\n",
    "# Initialise model\n",
    "model = SorscherRNN(pc_ensembles, Ng=params['Ng'], Np=params['Np'])\n",
    "model.to(device)\n",
    "print(f\"{model = }\")\n",
    "\n",
    "# Initialise optimizer (use custom weight decay, rather than torch optim decay)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'], betas=(0.9, 0.999), \\\n",
    "                             eps=1e-08, weight_decay=0.0, amsgrad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ee8580-f195-4621-a75c-6ad55e76a58d",
   "metadata": {},
   "source": [
    "### Train/Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bad08929-98de-4862-adcb-460f5fb06c23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=1/3000, loss(F,N)=6.3066, 6.3057, error(P,T)=0.9615,0.0413:   0%| | 2/3000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# whether to train\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train\u001b[38;5;241m:=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 19\u001b[0m     logger \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mnepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnepochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msave_freq\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpaths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# (re*)load checkpoint strs and latest checkpoint\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     checkpoint_filenames \u001b[38;5;241m=\u001b[39m filenames(paths[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoints\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/Dropbox/PhD BI-KI/Prosjekter/emergent-grid-cells/notebooks/../src/Models/SorscherRNN.py:206\u001b[0m, in \u001b[0;36mSorscherRNN.train\u001b[0;34m(self, trainloader, optimizer, weight_decay, nepochs, save_freq, paths, logger)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;66;03m# generic torch training loop\u001b[39;00m\n\u001b[1;32m    205\u001b[0m     logger\u001b[38;5;241m.\u001b[39mnew_epoch()\n\u001b[0;32m--> 206\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels, positions, indices \u001b[38;5;129;01min\u001b[39;00m trainloader:\n\u001b[1;32m    207\u001b[0m         indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(indices)\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;66;03m# zero the parameter gradients\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ml/lib/python3.9/site-packages/torch/utils/data/dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 521\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    524\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    525\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/ml/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1186\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1186\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1189\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ml/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1152\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1149\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1151\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1152\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1153\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1154\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/ml/lib/python3.9/site-packages/torch/utils/data/dataloader.py:990\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    977\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m    978\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m    979\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    987\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m    988\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m    989\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 990\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    991\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m    992\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    993\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m    994\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m    995\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ml/lib/python3.9/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[0;32m~/anaconda3/envs/ml/lib/python3.9/multiprocessing/connection.py:262\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ml/lib/python3.9/multiprocessing/connection.py:429\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 429\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m~/anaconda3/envs/ml/lib/python3.9/multiprocessing/connection.py:936\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    933\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 936\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    937\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    938\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m~/anaconda3/envs/ml/lib/python3.9/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "logger = None\n",
    "checkpoint_filenames = filenames(paths['checkpoints'])\n",
    "\n",
    "if checkpoint_filenames:\n",
    "    # load model latest (wrt. #epochs trained)\n",
    "    print(f\"Loading model at epoch = {checkpoint_filenames[-1]}\")\n",
    "    checkpoint = torch.load(paths['checkpoints'] / checkpoint_filenames[-1])\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    with open(paths[\"experiment\"] / \"logger.pkl\", \"rb\") as f:\n",
    "        logger = pickle.load(f)\n",
    "    print(\"Loaded weights\")\n",
    "    \n",
    "    if isinstance(datasampler, CLSampler):\n",
    "        datasampler.epoch_counter = len(loss_history)\n",
    "    \n",
    "# whether to train\n",
    "if train:=True:\n",
    "    logger = model.train(trainloader = dataloader, optimizer = optimizer, weight_decay=params['weight_decay'], \\\n",
    "                               nepochs=params['nepochs'], save_freq=params['save_freq'], paths=paths, logger=logger)\n",
    "    # (re*)load checkpoint strs and latest checkpoint\n",
    "    checkpoint_filenames = filenames(paths['checkpoints'])\n",
    "    checkpoint = torch.load(paths['checkpoints'] / checkpoint_filenames[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a35a08-d3f8-40a0-9be7-ddc9b010f8c4",
   "metadata": {},
   "source": [
    "## **Analyse Model**\n",
    "### Training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4dd377-770d-4347-b54b-703b24a03183",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=len(logger.training_metrics['familiar']) + 1,figsize=(8*7,8))\n",
    "ax[0].plot(logger.loss_history['familiar'], label='familiar')\n",
    "ax[0].plot(logger.training_metrics['familiar']['entropy'], label='optima')\n",
    "ax[0].set_title('Total-loss history')\n",
    "ax[0].legend()\n",
    "\n",
    "for env_type in [\"familiar\"]:\n",
    "    for i,(key,value) in enumerate(logger.training_metrics[env_type].items()):\n",
    "        ax[i+1].plot(value, label=env_type)\n",
    "        ax[i+1].set_title(key)\n",
    "        if key == 'KL':\n",
    "            ax[i+1].axhline(0,ls=\":\")\n",
    "        if key == 'pred_error':\n",
    "            ax[i+1].axhline(np.mean(logger.training_metrics[env_type]['true_error']),ls=\":\")\n",
    "        if key == 'CE':\n",
    "            ax[i+1].plot(logger.training_metrics[env_type]['entropy'], label='optima')\n",
    "        ax[i+1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# maximum labelled distribution entropy (uniform labelled distribution)\n",
    "n = 512\n",
    "px = np.ones(n) / n # uniform\n",
    "entropy = lambda x: -np.sum(x * np.log(x))\n",
    "print(f\"Maximum Entropy possible: {entropy(px)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083d7c36-c6fe-40ff-9747-fb9103e84fbe",
   "metadata": {},
   "source": [
    "### Create ratemaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a51a812-6e9c-42c3-890c-27c7d28339e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ratemap_trajectories(dataset, environment_idx, num_trajectories=1500):\n",
    "    \"\"\"Generate trajectories to compute ratemaps with (scipy.stats.binned_statistics_2d)\"\"\"\n",
    "    batch_velocities, batch_init_pc_positions, batch_positions = [], [], []\n",
    "    for _ in range(num_trajectories):\n",
    "        (velocities, init_pc_positions), _, positions, _ = dataset[environment_idx]\n",
    "        batch_velocities.append(velocities)\n",
    "        batch_init_pc_positions.append(init_pc_positions)\n",
    "        batch_positions.append(positions)\n",
    "    batch_inputs = [torch.stack(batch_velocities), torch.stack(batch_init_pc_positions)]\n",
    "    batch_positions = torch.stack(batch_positions).detach().numpy()\n",
    "    x = np.ravel(batch_positions[:,1:,0])\n",
    "    y = np.ravel(batch_positions[:,1:,1])\n",
    "    return batch_inputs, x, y\n",
    "\n",
    "def ratemaps_all_checkpoints(missing_ratemaps_filenames, epoch_ratemaps_env_path, batch_inputs, x, y):\n",
    "    # loop all saved model weights - and create ratemaps\n",
    "    for checkpoint_epoch in tqdm.tqdm(missing_ratemaps_filenames):\n",
    "        epoch_ratemaps_path = epoch_ratemaps_env_path / (checkpoint_epoch + \".pkl\")\n",
    "        \n",
    "        # load model weights, forward (to g) and compute ratemaps\n",
    "        checkpoint = torch.load(paths['checkpoints'] / checkpoint_epoch)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        activities = model.g(batch_inputs).detach().cpu().numpy()\n",
    "        activities = activities.reshape(-1, activities.shape[-1]).T\n",
    "        ratemaps = scipy.stats.binned_statistic_2d(x, y, activities, bins=res)[0]\n",
    "        \n",
    "        # save ratemaps\n",
    "        with open(epoch_ratemaps_path, \"wb\") as f:\n",
    "            pickle.dump(ratemaps,f)\n",
    "\n",
    "# mec_idxs = slice(0, params['Ng'], 1)\n",
    "res = np.array([64, 64])\n",
    "for env_i in range(max(len(environments),3)):\n",
    "    print(f\"Creating ratemaps for environment_idx = {env_i+1}/{max(len(environments),3)}\")\n",
    "    # create list of missing ratemaps - to be created\n",
    "    ratemaps_filenames = filenames(paths[\"ratemaps\"] / f\"env_{env_i}\")\n",
    "    ratemaps_filenames = [ratemaps_filename.split('.')[0] for ratemaps_filename in ratemaps_filenames]\n",
    "    missing_ratemaps_filenames = list(set(checkpoint_filenames) - set(ratemaps_filenames))\n",
    "    missing_ratemaps_filenames.sort()\n",
    "    if not missing_ratemaps_filenames:\n",
    "        continue\n",
    "    \n",
    "    # generate trajectories to compute ratemaps for\n",
    "    batch_inputs, x, y = ratemap_trajectories(dataset, environment_idx=env_i, num_trajectories=1500)\n",
    "    \n",
    "    epoch_ratemaps_env_path = paths[\"ratemaps\"] / f\"env_{env_i}\"\n",
    "    # loop all saved model weights - and create ratemaps\n",
    "    ratemaps_all_checkpoints(missing_ratemaps_filenames,\\\n",
    "                             epoch_ratemaps_env_path,\\\n",
    "                             batch_inputs\\\n",
    "                             , x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced28740-867e-43e8-9f75-3031d9cd227b",
   "metadata": {},
   "source": [
    "### Create ratemaps for novel environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c3a0b8-31db-4ac6-b22e-f8ef97cb40d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "novel_environments, novel_agents, novel_pc_ensembles = Experiment.get_default_ecology(seed = 23031994)\n",
    "novel_dataset = Dataset(agents = novel_agents, pc_ensembles = novel_pc_ensembles, num_samples = num_samples, **params)\n",
    "\n",
    "print(f\"Creating ratemaps for novel environment_idx = 1/1\")\n",
    "# create list of missing ratemaps - to be created\n",
    "epoch_ratemaps_env_path = paths[\"ratemaps\"] / \"env_novel\"\n",
    "if not epoch_ratemaps_env_path.exists():\n",
    "    os.makedirs(epoch_ratemaps_env_path)\n",
    "ratemaps_filenames = filenames(paths[\"ratemaps\"] / \"env_novel\")\n",
    "ratemaps_filenames = [ratemaps_filename.split('.')[0] for ratemaps_filename in ratemaps_filenames]\n",
    "missing_ratemaps_filenames = list(set(checkpoint_filenames) - set(ratemaps_filenames))\n",
    "missing_ratemaps_filenames.sort()\n",
    "\n",
    "if missing_ratemaps_filenames:\n",
    "    # generate trajectories to compute ratemaps for\n",
    "    batch_inputs, x, y = ratemap_trajectories(dataset, environment_idx=0, num_trajectories=1500)\n",
    "    # loop all saved model weights - and create ratemaps\n",
    "    ratemaps_all_checkpoints(missing_ratemaps_filenames,\\\n",
    "                             epoch_ratemaps_env_path,\\\n",
    "                             batch_inputs,\\\n",
    "                             x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7561c600-522c-4111-8981-149ae1edd6e7",
   "metadata": {},
   "source": [
    "### Load ratemaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073e21eb-f4f1-44b8-afba-0207d9da97ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_i = 0\n",
    "latest_ratemaps_path = paths[\"ratemaps\"] / f\"env_{env_i}\" / (checkpoint_filenames[-1] + \".pkl\")\n",
    "with open(latest_ratemaps_path, \"rb\") as f:\n",
    "    ratemaps = pickle.load(f)\n",
    "print(\"Loaded ratemaps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b8b772-08dc-4bc6-9ba7-8e2ec9ff4c17",
   "metadata": {},
   "source": [
    "### Example ratemaps (unsorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53afa647-437b-443f-ad26-b21e672f2d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_idx = 256*1\n",
    "num_ratemaps = 256\n",
    "fig, ax = multiimshow(ratemaps[start_idx:start_idx+num_ratemaps])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f55dccd-4149-4007-beb5-9a489cc13090",
   "metadata": {},
   "source": [
    "### Grid score ratemaps (only last model checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8224284b-0616-4c46-8693-34323490a47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_ratemaps(ratemaps, environment, env_name):\n",
    "    # if grid scores have already been calculate and saved to disk, load these (FAST!)\n",
    "    score_map_path = paths[\"grid_scores\"] / (checkpoint_filenames[-1] + env_name + \".pkl\")\n",
    "    if os.path.exists(score_map_path):\n",
    "        print(os.path.split(score_map_path)[-1], \"already exists. Loading score map.\")\n",
    "        with open(score_map_path, \"rb\") as f:\n",
    "            score_map = pickle.load(f)\n",
    "        return score_map\n",
    "    \n",
    "    # BANINO (and Sorscher) grid scoring method\n",
    "    from scores import GridScorer\n",
    "    res = np.array([32,32])\n",
    "    starts = [0.2] * 10\n",
    "    ends = np.linspace(0.4, 1.0, num=10)\n",
    "    box_width, box_height = environment.boxsize\n",
    "    coord_range=((environment.origo[0], box_width), (environment.origo[0], box_height))\n",
    "    coords_range=((-box_width/2, box_width/2), (-box_height/2, box_height/2))\n",
    "    mask_parameters = zip(starts, ends.tolist())\n",
    "    scorer = GridScorer(nbins=res[0], coords_range=coords_range, mask_parameters=mask_parameters)\n",
    "\n",
    "    # score ratemaps (Could use NUMBA for faster computations?)\n",
    "    score_map = np.zeros(params['Ng']) # ratemap score lookup table\n",
    "    for i in tqdm.trange(params['Ng']):\n",
    "        # interpolated_ratemap = interpolate_missing_pixels(ratemaps[i], np.isnan(ratemaps[i]))\n",
    "        # score_map[i] = sm.gridness(interpolated_ratemap)\n",
    "        score_60, score_90, max_60_mask, max_90_mask, sac = scorer.get_scores(ratemaps[i])\n",
    "        score_map[i] = score_60\n",
    "\n",
    "    with open(score_map_path, \"wb\") as f:\n",
    "        pickle.dump(score_map,f)\n",
    "    return score_map\n",
    "\n",
    "# create scores for all ratemaps across all environments\n",
    "for env_i in range(max(len(environments),3)):\n",
    "    print(f\"Calculating grid score for the ratemaps of environment={env_i+1}/{max(len(environments),3)}\")\n",
    "    latest_ratemaps_path_env_i = paths[\"ratemaps\"] / f\"env_{env_i}\" / (checkpoint_filenames[-1] + \".pkl\")\n",
    "    with open(latest_ratemaps_path_env_i, \"rb\") as f:\n",
    "        ratemaps_env_i = pickle.load(f)\n",
    "    score_map = score_ratemaps(ratemaps_env_i, environments[env_i], f\"_env_{env_i}\")\n",
    "# create scores for all ratemaps of novel environment\n",
    "print(f\"Calculating grid score for novel environment\")\n",
    "with open(paths[\"ratemaps\"] / \"env_novel\" / (checkpoint_filenames[-1] + \".pkl\"), \"rb\") as f:\n",
    "    ratemaps_env_novel = pickle.load(f)\n",
    "score_map_env_novel = score_ratemaps(ratemaps_env_novel, novel_environments[0], \"_env_novel\")\n",
    "\n",
    "# choose score map\n",
    "env_i = 0\n",
    "score_map = score_ratemaps(ratemaps, environments[env_i], f\"_env_{env_i}\")\n",
    "\n",
    "# sort scores and ratemaps\n",
    "sort_idxs = np.argsort(score_map)[::-1]\n",
    "sorted_scores = score_map[sort_idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c730d0a1-1840-4d05-a317-edbaa306196c",
   "metadata": {},
   "source": [
    "### Example ratemaps (sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5ac0f6-29cc-4b3a-a56f-3ce230f16685",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_idx = 0\n",
    "num_ratemaps = 2**6\n",
    "fig, axs = multiimshow(ratemaps[sort_idxs][start_idx:start_idx+num_ratemaps], titles=sort_idxs[start_idx:start_idx+num_ratemaps])\n",
    "fig.suptitle(f\"gc_score = {sorted_scores[start_idx]} -- {sorted_scores[start_idx+num_ratemaps]}\", fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9e51f5-a124-4dd4-ad9d-c54a553845e5",
   "metadata": {},
   "source": [
    "### Create grid cell ratemap dynamics (grid cell emergence video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc73e6bb-dc03-419f-9aa5-56c51c5a4914",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if create_videos:=False:\n",
    "    from dynamics import RatemapDynamics\n",
    "    for env_i in range(len(environments)):\n",
    "        print(f\"Creating grid dynamics for the ratemaps of environment={env_i+1}/{len(environments)}\")\n",
    "\n",
    "        # concatenating ratemaps\n",
    "        ratemap_dynamics = []\n",
    "        print(\"Loading ratemaps\")\n",
    "        for checkpoint_epoch in tqdm.tqdm(checkpoint_filenames):\n",
    "            with open(paths[\"ratemaps\"] / f\"env_{env_i}\" / (checkpoint_epoch + \".pkl\"), \"rb\") as f:\n",
    "                # load and sort ratemaps wrt. grid score\n",
    "                ratemap_dynamics.append(pickle.load(f)[sort_idxs])\n",
    "        ratemap_dynamics = np.stack(ratemap_dynamics, axis=0)\n",
    "\n",
    "        # creating videos from ratemaps\n",
    "        num_gc_in_video = 64\n",
    "        num_videos = params['Ng'] // num_gc_in_video\n",
    "        print(\"Creating videos\")\n",
    "        for gcs_i in tqdm.trange(0, num_videos):\n",
    "            idxs = slice(gcs_i * num_gc_in_video, (gcs_i + 1) * num_gc_in_video)\n",
    "            if os.path.exists(paths[\"dynamics\"] / f\"env_{env_i}\" / f'{idxs.start}-{idxs.stop - 1}.mp4'):\n",
    "                continue\n",
    "            ratemap_video = RatemapDynamics(ratemap_dynamics[:,idxs], sorted_scores[idxs], sort_idxs[idxs], epochs = checkpoint_filenames)\n",
    "            ratemap_video.animation.save(paths[\"dynamics\"] / f\"env_{env_i}\" / f'{idxs.start}-{idxs.stop - 1}.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a40c4a6-f727-4fd7-928c-a583c08ea080",
   "metadata": {},
   "source": [
    "### Decoding labels and predictions to cartesian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737f3b6b-92cc-412c-b038-11bb8e02cbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_idx = 0\n",
    "[[vel, init_pos], labels, positions, index] = dataset[environment_idx]\n",
    "true_cartesian_pos = positions\n",
    "true_decoded_pos = pc_ensembles[environment_idx].to_euclid(torch.cat([init_pos[None], labels]))\n",
    "pc_preds = model([vel, init_pos]).detach().cpu()[0]\n",
    "predicted_decoded_pos = pc_ensembles[environment_idx].to_euclid(torch.cat([init_pos[None], pc_preds]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888cd951-d0ca-47f7-93df-a48bd4cd6a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(*true_cartesian_pos.T, label='true_cartesian_pos')\n",
    "ax.plot(*true_decoded_pos.T, label='true_decoded_pos')\n",
    "ax.plot(*predicted_decoded_pos.T, label='predicted_decoded_pos', ls=':')\n",
    "environments[environment_idx].plot_board(ax)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f961bd5c-05ce-4ec0-9e89-9845243e38cb",
   "metadata": {},
   "source": [
    "### Plot all place cell centers and some with tuning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c0284b-16f7-401d-af6a-1e042e2b4cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "x,y = pc_ensembles[environment_idx].pcs.T\n",
    "\n",
    "ax.plot(x, y, \"+\")\n",
    "# add standard deviation circles to locations\n",
    "for i in range(5):\n",
    "    ax.plot(x[i], y[i], \"r+\")\n",
    "    a_circle = plt.Circle((x[i], y[i]), pc_ensembles[0].pc_width, fill=False, color=(1, 0, 0, 0.5))\n",
    "    ax.add_artist(a_circle)\n",
    "\n",
    "plt.title(\"Spatial plot of place cell locations\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afd4981-2870-4fca-a590-ae8b55792942",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
