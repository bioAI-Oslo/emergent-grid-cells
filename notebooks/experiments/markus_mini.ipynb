{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be522a85-71a1-4918-a711-f493aefb26ad",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "996cfee3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#%matplotlib notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f109164",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Computational packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import tqdm\n",
    "\n",
    "# General packages\n",
    "import datetime\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"../../src\") if \"../../src\" not in sys.path else None \n",
    "\n",
    "# custom packages\n",
    "import ratsimulator\n",
    "from ratsimulator.Environment import Rectangle\n",
    "\n",
    "# avoid adding multiple relave paths to sys.path\n",
    "\n",
    "from PlaceCells import PlaceCells\n",
    "from Models import SorscherRNN\n",
    "from Experiment import Experiment\n",
    "from methods import *\n",
    "from datahandling import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fb2b7a-33d6-499f-b899-7f249416b36d",
   "metadata": {},
   "source": [
    "### Setup Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57ff00f3-81cd-4031-8e86-28d9955f6572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment <3CL> is NEW. Loading DEFAULT experiment settings!\n",
      "Singular matrix\n",
      "Singular matrix\n",
      "Default <params>, <environments>, <agents> and <pc_ensembles> can now be changed. Finish setup by calling setup()\n",
      "Experiment <10ME_small> is NEW. Loading DEFAULT experiment settings!\n",
      "Singular matrix\n",
      "Singular matrix\n",
      "Default <params>, <environments>, <agents> and <pc_ensembles> can now be changed. Finish setup by calling setup()\n",
      "Singular matrix\n",
      "Singular matrix\n",
      "Singular matrix\n",
      "Singular matrix\n",
      "Singular matrix\n",
      "Singular matrix\n",
      "Singular matrix\n",
      "Singular matrix\n",
      "Singular matrix\n",
      "Singular matrix\n",
      "Singular matrix\n",
      "Singular matrix\n",
      "Singular matrix\n",
      "Singular matrix\n",
      "Singular matrix\n",
      "Singular matrix\n",
      "Singular matrix\n",
      "Singular matrix\n",
      "Creating directories\n",
      "Saving experiment details\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment = Experiment(name = '200ME_small')\n",
    "if experiment.is_new_experiment:\n",
    "    \"\"\"\n",
    "    MAKE EXPERIMENTAL CHANGES HERE\n",
    "    \"\"\"\n",
    "\n",
    "    if experiment.is_new_experiment:\n",
    "        experiment.params['nepochs'] = 1000\n",
    "        # add them gcs\n",
    "        for i in range(1,200):\n",
    "            environments, agents, pc_ensembles = experiment.get_default_ecology(seed = i)\n",
    "            experiment.environments += environments\n",
    "            experiment.agents += agents\n",
    "            experiment.pc_ensembles += pc_ensembles\n",
    "\n",
    "experiment.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "530b5b7b-f788-45d5-a318-61e2cab433a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device=device(type='cpu')\n"
     ]
    }
   ],
   "source": [
    "# detach experiment specifics\n",
    "params = experiment.params\n",
    "environments = experiment.environments\n",
    "agents = experiment.agents\n",
    "pc_ensembles = experiment.pc_ensembles\n",
    "paths = experiment.paths\n",
    "\n",
    "num_workers = 30\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"{device=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626330b4-f133-4ac8-bc0f-2c75b3ae70dd",
   "metadata": {},
   "source": [
    "### Initialise objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cff29f0-2cb5-4eb8-9ece-72a72fb91141",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model = SorscherRNN(\n",
      "  (init_position_encoder): Linear(in_features=522, out_features=500, bias=False)\n",
      "  (RNN): RNN(2, 500, bias=False, batch_first=True)\n",
      "  (decoder): Linear(in_features=500, out_features=512, bias=False)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/m/miniforge3/envs/tf/lib/python3.8/site-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 30 worker processes in total. Our suggested max number of worker in current system is 8 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "# Initialise data loading\n",
    "num_samples = params['nsteps'] * params['batch_size'] # * params['nepochs']\n",
    "dataset = Dataset(agents = agents, pc_ensembles = pc_ensembles, num_samples = num_samples, context = True, **params)\n",
    "datasampler = eval(params['sampler'])(num_environments = len(environments), num_samples = num_samples)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=params['batch_size'], sampler = datasampler, num_workers=num_workers)\n",
    "\n",
    "# Initialise model\n",
    "model = SorscherRNN(pc_ensembles, Ng=params['Ng'], Np=params['Np'], context = True)\n",
    "model.to(device)\n",
    "print(f\"{model = }\")\n",
    "\n",
    "# Initialise optimizer (use custom weight decay, rather than torch optim decay)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'], betas=(0.9, 0.999), \\\n",
    "                             eps=1e-08, weight_decay=0.0, amsgrad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ee8580-f195-4621-a75c-6ad55e76a58d",
   "metadata": {},
   "source": [
    "### Train/Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bad08929-98de-4862-adcb-460f5fb06c23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                              | 0/1000 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "Caught UnboundLocalError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/Users/m/miniforge3/envs/tf/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/Users/m/miniforge3/envs/tf/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/Users/m/miniforge3/envs/tf/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/Users/m/Documents/skole/emergent-grid-cells/notebooks/experiments/../../src/datahandling.py\", line 71, in __getitem__\n    if context:\nUnboundLocalError: local variable 'context' referenced before assignment\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/rv/mky69fln76nbwl7qkb06318h0000gn/T/ipykernel_15693/1074206188.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# whether to train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m:=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     loss_history, training_metrics = model.train(trainloader = dataloader, optimizer = optimizer, weight_decay=params['weight_decay'], \\\n\u001b[0m\u001b[1;32m     21\u001b[0m                                \u001b[0mnepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nepochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'checkpoints'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                                loss_history = loss_history, training_metrics = training_metrics)\n",
      "\u001b[0;32m~/Documents/skole/emergent-grid-cells/notebooks/experiments/../../src/Models/SorscherRNN.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, trainloader, optimizer, weight_decay, nepochs, checkpoint_path, loss_history, training_metrics)\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0mrunning_ce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_entropy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_KL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_l2_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mrunning_pred_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_true_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m                 \u001b[0;31m# zero the parameter gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tf/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tf/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1201\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tf/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1227\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1229\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1230\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tf/lib/python3.8/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: Caught UnboundLocalError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/Users/m/miniforge3/envs/tf/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/Users/m/miniforge3/envs/tf/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/Users/m/miniforge3/envs/tf/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/Users/m/Documents/skole/emergent-grid-cells/notebooks/experiments/../../src/datahandling.py\", line 71, in __getitem__\n    if context:\nUnboundLocalError: local variable 'context' referenced before assignment\n"
     ]
    }
   ],
   "source": [
    "loss_history = []\n",
    "training_metrics = {}\n",
    "checkpoint_filenames = filenames(paths['checkpoints'])\n",
    "\n",
    "if checkpoint_filenames:\n",
    "    # load model latest (wrt. #epochs trained)\n",
    "    print(f\"Loading model at epoch = {checkpoint_filenames[-1]}\")\n",
    "    checkpoint = torch.load(paths['checkpoints'] / checkpoint_filenames[-1])\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    loss_history = checkpoint['loss_history']\n",
    "    training_metrics = checkpoint['training_metrics']\n",
    "    print(\"Loaded weights\")\n",
    "    \n",
    "    if isinstance(datasampler, CLSampler):\n",
    "        datasampler.epoch_counter = len(loss_history)\n",
    "    \n",
    "# whether to train\n",
    "if train:=True:\n",
    "    loss_history, training_metrics = model.train(trainloader = dataloader, optimizer = optimizer, weight_decay=params['weight_decay'], \\\n",
    "                               nepochs=params['nepochs'], checkpoint_path = paths['checkpoints'], \\\n",
    "                               loss_history = loss_history, training_metrics = training_metrics)\n",
    "    # (re)load checkpoint strs and latest checkpoint\n",
    "    checkpoint_filenames = filenames(paths['checkpoints'])\n",
    "    checkpoint = torch.load(paths['checkpoints'] / checkpoint_filenames[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a35a08-d3f8-40a0-9be7-ddc9b010f8c4",
   "metadata": {},
   "source": [
    "## **Analyse Model**\n",
    "### Training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4dd377-770d-4347-b54b-703b24a03183",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=len(training_metrics) + 1,figsize=(15,4))\n",
    "ax[0].plot(loss_history)\n",
    "ax[0].plot(training_metrics['entropy'], label='optima')\n",
    "ax[0].set_title('Total-loss history')\n",
    "ax[0].legend()\n",
    "\n",
    "for i,(key,value) in enumerate(training_metrics.items()):\n",
    "    ax[i+1].plot(value)\n",
    "    ax[i+1].set_title(key)\n",
    "    if key == 'KL':\n",
    "        ax[i+1].axhline(0,ls=\":\")\n",
    "    if key == 'pred_error':\n",
    "        ax[i+1].axhline(np.mean(training_metrics['true_error']),ls=\":\")\n",
    "    if key == 'CE':\n",
    "        ax[i+1].plot(training_metrics['entropy'], label='optima')\n",
    "        ax[i+1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# maximum labelled distribution entropy (uniform labelled distribution)\n",
    "n = 512\n",
    "px = np.ones(n) / n # uniform\n",
    "entropy = lambda x: -np.sum(x * np.log(x))\n",
    "print(f\"Maximum Entropy possible: {entropy(px)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083d7c36-c6fe-40ff-9747-fb9103e84fbe",
   "metadata": {},
   "source": [
    "### Create ratemaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a51a812-6e9c-42c3-890c-27c7d28339e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ratemap_trajectories(dataset, environment_idx, num_trajectories=1500):\n",
    "    \"\"\"Generate trajectories to compute ratemaps with (scipy.stats.binned_statistics_2d)\"\"\"\n",
    "    batch_velocities, batch_init_pc_positions, batch_positions = [], [], []\n",
    "    for _ in range(num_trajectories):\n",
    "        (velocities, init_pc_positions), _, positions, _ = dataset[environment_idx]\n",
    "        batch_velocities.append(velocities)\n",
    "        batch_init_pc_positions.append(init_pc_positions)\n",
    "        batch_positions.append(positions)\n",
    "    batch_inputs = [torch.stack(batch_velocities), torch.stack(batch_init_pc_positions)]\n",
    "    batch_positions = torch.stack(batch_positions).detach().numpy()\n",
    "    x = np.ravel(batch_positions[:,1:,0])\n",
    "    y = np.ravel(batch_positions[:,1:,1])\n",
    "    return batch_inputs, x, y\n",
    "\n",
    "# mec_idxs = slice(0, params['Ng'], 1)\n",
    "res = np.array([32, 32])\n",
    "for env_i in range(len(environments)):\n",
    "    print(f\"Creating ratemaps for environment_idx = {env_i+1}/{len(environments)}\")\n",
    "    # create list of missing ratemaps - to be created\n",
    "    ratemaps_filenames = filenames(paths[\"ratemaps\"] / f\"env_{env_i}\")\n",
    "    ratemaps_filenames = [ratemaps_filename.split('.')[0] for ratemaps_filename in ratemaps_filenames]\n",
    "    missing_ratemaps_filenames = list(set(checkpoint_filenames) - set(ratemaps_filenames))\n",
    "    missing_ratemaps_filenames.sort()\n",
    "    if not missing_ratemaps_filenames:\n",
    "        continue\n",
    "    \n",
    "    # generate trajectories to compute ratemaps for\n",
    "    batch_inputs, x, y = ratemap_trajectories(dataset, environment_idx=env_i, num_trajectories=1500)\n",
    "    \n",
    "    # loop all saved model weights - and create ratemaps\n",
    "    for checkpoint_epoch in tqdm.tqdm(missing_ratemaps_filenames):\n",
    "        epoch_ratemaps_path = paths[\"ratemaps\"] / f\"env_{env_i}\" / (checkpoint_epoch + \".pkl\")\n",
    "        \n",
    "        # load model weights, forward (to g) and compute ratemaps\n",
    "        checkpoint = torch.load(paths['checkpoints'] / checkpoint_epoch)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        activities = model.g(batch_inputs).detach().cpu().numpy()\n",
    "        activities = activities.reshape(-1, activities.shape[-1]).T\n",
    "        ratemaps = scipy.stats.binned_statistic_2d(x, y, activities, bins=res)[0]\n",
    "        \n",
    "        # save ratemaps\n",
    "        with open(epoch_ratemaps_path, \"wb\") as f:\n",
    "            pickle.dump(ratemaps,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7561c600-522c-4111-8981-149ae1edd6e7",
   "metadata": {},
   "source": [
    "### Load ratemaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073e21eb-f4f1-44b8-afba-0207d9da97ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_i = 0\n",
    "latest_ratemaps_path = paths[\"ratemaps\"] / f\"env_{env_i}\" / (checkpoint_filenames[-1] + \".pkl\")\n",
    "with open(latest_ratemaps_path, \"rb\") as f:\n",
    "    ratemaps = pickle.load(f)\n",
    "print(\"Loaded ratemaps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b8b772-08dc-4bc6-9ba7-8e2ec9ff4c17",
   "metadata": {},
   "source": [
    "### Example ratemaps (unsorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53afa647-437b-443f-ad26-b21e672f2d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_idx = 256*1\n",
    "num_ratemaps = 256\n",
    "fig, ax = multiimshow(ratemaps[start_idx:start_idx+num_ratemaps])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f55dccd-4149-4007-beb5-9a489cc13090",
   "metadata": {},
   "source": [
    "### Grid score ratemaps (only last model checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8224284b-0616-4c46-8693-34323490a47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_ratemaps(ratemaps, env_i):\n",
    "    # if grid scores have already been calculate and saved to disk, load these (FAST!)\n",
    "    score_map_path = paths[\"grid_scores\"] / (checkpoint_filenames[-1] + f\"_env_{env_i}.pkl\")\n",
    "    if os.path.exists(score_map_path):\n",
    "        print(os.path.split(score_map_path)[-1], \"already exists. Loading score map.\")\n",
    "        with open(score_map_path, \"rb\") as f:\n",
    "            score_map = pickle.load(f)\n",
    "        return score_map\n",
    "    \n",
    "    # BANINO (and Sorscher) grid scoring method\n",
    "    from scores import GridScorer\n",
    "    res = np.array([32,32])\n",
    "    starts = [0.2] * 10\n",
    "    ends = np.linspace(0.4, 1.0, num=10)\n",
    "    box_width, box_height = environments[env_i].boxsize\n",
    "    coord_range=((environments[env_i].origo[0], box_width), (environments[env_i].origo[0], box_height))\n",
    "    coords_range=((-box_width/2, box_width/2), (-box_height/2, box_height/2))\n",
    "    mask_parameters = zip(starts, ends.tolist())\n",
    "    scorer = GridScorer(nbins=res[0], coords_range=coords_range, mask_parameters=mask_parameters)\n",
    "\n",
    "    # score ratemaps (Could use NUMBA for faster computations?)\n",
    "    score_map = np.zeros(params['Ng']) # ratemap score lookup table\n",
    "    for i in tqdm.trange(params['Ng']):\n",
    "        # interpolated_ratemap = interpolate_missing_pixels(ratemaps[i], np.isnan(ratemaps[i]))\n",
    "        # score_map[i] = sm.gridness(interpolated_ratemap)\n",
    "        score_60, score_90, max_60_mask, max_90_mask, sac = scorer.get_scores(ratemaps[i])\n",
    "        score_map[i] = score_60\n",
    "\n",
    "    with open(score_map_path, \"wb\") as f:\n",
    "        pickle.dump(score_map,f)\n",
    "    return score_map\n",
    "\n",
    "# create scores for all ratemaps across all environments\n",
    "for env_i in range(len(environments)):\n",
    "    print(f\"Calculating grid score for the ratemaps of environment={env_i+1}/{len(environments)}\")\n",
    "    score_map = score_ratemaps(ratemaps, env_i)\n",
    "\n",
    "# choose score map\n",
    "score_map = score_ratemaps(ratemaps, env_i = 0)\n",
    "\n",
    "# sort scores and ratemaps\n",
    "sort_idxs = np.argsort(score_map)[::-1]\n",
    "sorted_scores = score_map[sort_idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c730d0a1-1840-4d05-a317-edbaa306196c",
   "metadata": {},
   "source": [
    "### Example ratemaps (sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5ac0f6-29cc-4b3a-a56f-3ce230f16685",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_idx = 0\n",
    "num_ratemaps = 2**6\n",
    "fig, axs = multiimshow(ratemaps[sort_idxs][start_idx:start_idx+num_ratemaps], titles=sort_idxs[start_idx:start_idx+num_ratemaps])\n",
    "fig.suptitle(f\"gc_score = {sorted_scores[start_idx]} -- {sorted_scores[start_idx+num_ratemaps]}\", fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9e51f5-a124-4dd4-ad9d-c54a553845e5",
   "metadata": {},
   "source": [
    "### Create grid cell ratemap dynamics (grid cell emergence video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc73e6bb-dc03-419f-9aa5-56c51c5a4914",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dynamics import RatemapDynamics\n",
    "for env_i in range(len(environments)):\n",
    "    print(f\"Creating grid dynamics for the ratemaps of environment={env_i+1}/{len(environments)}\")\n",
    "    \n",
    "    # concatenating ratemaps\n",
    "    ratemap_dynamics = []\n",
    "    print(\"Loading ratemaps\")\n",
    "    for checkpoint_epoch in tqdm.tqdm(checkpoint_filenames):\n",
    "        with open(paths[\"ratemaps\"] / f\"env_{env_i}\" / (checkpoint_epoch + \".pkl\"), \"rb\") as f:\n",
    "            # load and sort ratemaps wrt. grid score\n",
    "            ratemap_dynamics.append(pickle.load(f)[sort_idxs])\n",
    "    ratemap_dynamics = np.stack(ratemap_dynamics, axis=0)\n",
    "    \n",
    "    # creating videos from ratemaps\n",
    "    num_gc_in_video = 64\n",
    "    num_videos = params['Ng'] // num_gc_in_video\n",
    "    print(\"Creating videos\")\n",
    "    for gcs_i in tqdm.trange(0, num_videos):\n",
    "        idxs = slice(gcs_i * num_gc_in_video, (gcs_i + 1) * num_gc_in_video)\n",
    "        if os.path.exists(paths[\"dynamics\"] / f\"env_{env_i}\" / f'{idxs.start}-{idxs.stop - 1}.mp4'):\n",
    "            continue\n",
    "        ratemap_video = RatemapDynamics(ratemap_dynamics[:,idxs], sorted_scores[idxs], sort_idxs[idxs], epochs = checkpoint_filenames)\n",
    "        ratemap_video.animation.save(paths[\"dynamics\"] / f\"env_{env_i}\" / f'{idxs.start}-{idxs.stop - 1}.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897a7f22-e27d-4971-8115-1132ebf9f568",
   "metadata": {},
   "source": [
    "### Predicted place Cells (with(out) SOFTMAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5909fbd0-c6e8-47e6-aadd-46c3607fe07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "forward_with_softmax = lambda x: torch.exp(model(x, log_softmax=True))\n",
    "ratemaps = compute_ratemaps(model=model, dataset=dataset, num_trajectories=1250, res=res, idxs=idxs)[0]\n",
    "fig, ax = multiimshow(ratemaps)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfd1d49-da3f-4566-bdaa-d4e1b86d8dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.prune_mask = list(range(int(4096/2),4096)) # set prune mask\n",
    "model.prune_mask = [] # reset prune mask\n",
    "model.prune_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d860630b-994e-4cd1-be6a-60d62aaaed95",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs=slice(0, 16**2, 1)\n",
    "res=np.array([32, 32])\n",
    "ratemaps = compute_ratemaps(model=model.g, dataset=dataset, num_trajectories=1250, res=res, idxs=idxs)[0]\n",
    "fig, ax = multiimshow(ratemaps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a40c4a6-f727-4fd7-928c-a583c08ea080",
   "metadata": {},
   "source": [
    "### Decoding labels and predictions to cartesian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737f3b6b-92cc-412c-b038-11bb8e02cbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_idx = 0\n",
    "[[vel, init_pos], labels, positions, index] = dataset[environment_idx]\n",
    "true_cartesian_pos = positions\n",
    "true_decoded_pos = pc_ensembles[environment_idx].to_euclid(torch.cat([init_pos[None], labels]))\n",
    "pc_preds = model([vel, init_pos]).detach().cpu()[0]\n",
    "predicted_decoded_pos = pc_ensembles[environment_idx].to_euclid(torch.cat([init_pos[None], pc_preds]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888cd951-d0ca-47f7-93df-a48bd4cd6a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(*true_cartesian_pos.T, label='true_cartesian_pos')\n",
    "ax.plot(*true_decoded_pos.T, label='true_decoded_pos')\n",
    "ax.plot(*predicted_decoded_pos.T, label='predicted_decoded_pos', ls=':')\n",
    "environments[environment_idx].plot_board(ax)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f961bd5c-05ce-4ec0-9e89-9845243e38cb",
   "metadata": {},
   "source": [
    "### Plot all place cell centers and some with tuning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c0284b-16f7-401d-af6a-1e042e2b4cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "x,y = pc_ensembles[environment_idx].pcs.T\n",
    "\n",
    "ax.plot(x, y, \"+\")\n",
    "# add standard deviation circles to locations\n",
    "for i in range(5):\n",
    "    ax.plot(x[i], y[i], \"r+\")\n",
    "    a_circle = plt.Circle((x[i], y[i]), pc_ensembles[0].pc_width, fill=False, color=(1, 0, 0, 0.5))\n",
    "    ax.add_artist(a_circle)\n",
    "\n",
    "plt.title(\"Spatial plot of place cell locations\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ba8623-47eb-424d-95b9-6d80d79de68c",
   "metadata": {},
   "source": [
    "### Calculate grid scores using different implementations of the metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20675fc-dc2e-4ff7-87c1-5bff46d83dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom grid score\n",
    "print(\"CUSTOM:\", grid_score(ratemaps[1])) \n",
    "\n",
    "# CINPLA grid score\n",
    "import spatial_maps as sm\n",
    "print(\"CINPLA:\", sm.gridness(ratemaps[1])) \n",
    "\n",
    "# BANINO (and Sorscher) grid scoring\n",
    "from scores import GridScorer\n",
    "\"\"\"\n",
    "One difference from custom and CINPLA grid scores: \n",
    "1. Uses average difference between phase60 and phase30 correlations\n",
    "\"\"\"\n",
    "starts = [0.2] * 10\n",
    "ends = np.linspace(0.4, 1.0, num=10)\n",
    "coord_range=((0, environment.boxsize[0]), (0, environment.boxsize[1]))\n",
    "box_width, box_height = 2.2, 2.2\n",
    "coords_range=((-box_width/2, box_width/2), (-box_height/2, box_height/2))\n",
    "mask_parameters = zip(starts, ends.tolist())\n",
    "scorer = GridScorer(nbins=res[0], coords_range=coords_range, mask_parameters=mask_parameters)\n",
    "\n",
    "#score_60, score_90, max_60_mask, max_90_mask, sac, max_60_ind = zip(\n",
    "#      *[scorer.get_scores(rm.reshape(res, res)) for rm in tqdm(rate_map_lores)])\n",
    "score_60, score_90, max_60_mask, max_90_mask, sac = scorer.get_scores(ratemaps[1])\n",
    "print(\"BANINO/SORSCHER:\", score_60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80105ea-8683-4c30-a027-e0c958c4bfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose grid scoring function to use, e.g: grid_score, sm.gridness or scorer.get_scores\n",
    "# for scorer.get_scores use: < (lambda rm: scorer.get_scores(rm)[0])(rate_map) >\n",
    "grid_scoring_fn = lambda rate_map: sm.gridness(rate_map)\n",
    "\n",
    "#map(grid_scoring_fn, *ratemaps)\n",
    "grid_scoring_fn(ratemaps[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3cbc7d-5feb-4d5e-acdb-99c88f14b1fe",
   "metadata": {},
   "source": [
    "### Small analysis / checks / tests etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211373b1-2a56-49ea-b6c3-4ab5de3a088c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wr = model.recurrence.weight.detach().cpu().numpy()\n",
    "Wr = model.RNN.weight_hh_l0.detach().cpu().numpy()\n",
    "stats = lambda W : print(f\"{np.min(W)=}, {np.max(W)=}, {np.min(abs(W))=}, {np.mean(W)=}, {np.std(W)=}, {np.sum(W**2)=}\")\n",
    "stats(Wr)\n",
    "plt.imshow(Wr[:25,:25])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44ae245-f59f-4ee6-b3c3-6f5b8ab29a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "Wp = model.init_position_encoder.weight.detach().cpu().numpy()\n",
    "stats(Wp)\n",
    "plt.imshow(Wp)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afd4981-2870-4fca-a590-ae8b55792942",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "beed92cad6ab96925ef3cc1ff29b29d5ea2be663d6c20c3e6b7769e66d799522"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
