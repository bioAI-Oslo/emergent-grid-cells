{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "2e54a891-ce1e-42a5-aa3e-23f6b3bcffe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "720431e5-051d-4043-834f-7f641d82956c",
   "metadata": {},
   "outputs": [],
   "source": [
    "L, N, Hin, Hout = 1, 3, 2, 5\n",
    "n_rnns = 1\n",
    "rnn = torch.nn.RNN(Hin, Hout, n_rnns, batch_first=True, bias=False, nonlinearity='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "39a11c1b-990b-468b-8d94-725fee22a58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_weight = torch.eye(5)\n",
    "new_weight[0] = torch.zeros(5)\n",
    "new_weight[1,0] = torch.ones(1)\n",
    "rnn.weight_hh_l0 = torch.nn.parameter.Parameter(new_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "b13a1c95-bc93-4e21-8fe2-9b4e2ede940f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.ones(3)[:,None] - np.ones(2)[None]*2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "14fd1eb4-dffc-41c2-b854-56c78b73a1f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(np.arange(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "8ef124b8-f9bc-4be8-847a-0f88bf9d55f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2,3]\n",
    "a[(idx := 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "3ee6f440-5771-4292-bdad-93ec9e83c705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "grid = plt.GridSpec(3,3)\n",
    "fig.add_subplot(grid[0:2,0:2])\n",
    "fig.add_subplot(grid[0,2])\n",
    "fig.add_subplot(grid[1,2])\n",
    "fig.add_subplot(grid[2,0])\n",
    "fig.add_subplot(grid[2,1])\n",
    "fig.add_subplot(grid[2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "15865b17-a941-44f5-ab58-1bdfef704602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<AxesSubplot:>,\n",
       " <AxesSubplot:>,\n",
       " <AxesSubplot:>,\n",
       " <AxesSubplot:>,\n",
       " <AxesSubplot:>,\n",
       " <AxesSubplot:>]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig.axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "c86a94d2-be8f-4510-a375-e1150fa69e83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<Figure size 432x288 with 4 Axes>,\n",
       " array([[<AxesSubplot:>, <AxesSubplot:>],\n",
       "        [<AxesSubplot:>, <AxesSubplot:>]], dtype=object))"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig2,axs = plt.subplots(2,2)\n",
    "fig2, axs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "f38481c8-2504-49d4-addb-a1d2dc5566c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<Figure size 432x288 with 8 Axes>,\n",
       " array([[<AxesSubplot:>, <AxesSubplot:>],\n",
       "        [<AxesSubplot:>, <AxesSubplot:>]], dtype=object),\n",
       " array([[<AxesSubplot:>, <AxesSubplot:>],\n",
       "        [<AxesSubplot:>, <AxesSubplot:>]], dtype=object))"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(2,2)\n",
    "axs2 = fig.subplots(2,2)\n",
    "fig, axs, axs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "69a67b02-4169-4e7b-aece-2f690c4338da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1., 0.]], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.weight_hh_l0[[2,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "72c8de18-fa85-4bbc-8ac0-ed0713d9a92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 2}\n",
      "{(1, 'b'), (1, 'd'), (0, 'a'), (2, 'c')}\n"
     ]
    }
   ],
   "source": [
    "a = {0: 'a', 1: 'b'}\n",
    "b = {1: 'd', 2: 'c'}\n",
    "print(a.keys() ^ b.keys())\n",
    "print(a.items() ^ b.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "27cf4023-78c5-49e5-a56c-ccf7c706a4c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{(2, 'c'), (0, 'b')}\""
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.items() ^ a.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "ce651533-9004-4c11-a047-8cacdabe33f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__constants__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__jit_unused_properties__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_call_impl',\n",
       " '_get_backward_hooks',\n",
       " '_get_name',\n",
       " '_load_from_state_dict',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_named_members',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_version',\n",
       " 'add_module',\n",
       " 'all_weights',\n",
       " 'apply',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'check_forward_args',\n",
       " 'check_hidden_size',\n",
       " 'check_input',\n",
       " 'children',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'double',\n",
       " 'dump_patches',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'flatten_parameters',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'get_buffer',\n",
       " 'get_expected_hidden_size',\n",
       " 'get_extra_state',\n",
       " 'get_parameter',\n",
       " 'get_submodule',\n",
       " 'half',\n",
       " 'load_state_dict',\n",
       " 'modules',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'parameters',\n",
       " 'permute_hidden',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_parameter',\n",
       " 'requires_grad_',\n",
       " 'reset_parameters',\n",
       " 'set_extra_state',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'to',\n",
       " 'to_empty',\n",
       " 'train',\n",
       " 'type',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1d9f38-c512-426f-ba8c-e8e7647bd4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment1 - experiment2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "396cf786-79c4-4298-8877-334a689f5d20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139893060487936"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "081f0e3d-a53a-4b93-8ef6-f28b5eb9b03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0] = 'b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "51ffbd69-3828-484f-a340-b287aac3de8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0: 'b', 1: 'b'}, 139893060487936)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, id(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "76d9e662-e2f7-4275-911e-55620b226c60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 1.]], grad_fn=<CloneBackward0>)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.weight_hh_l0.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b1a2ec0d-5946-4cf0-9f96-ae3befcbac0c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1077970551.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_3109118/1077970551.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    for i in range(10) if True else pass:\u001b[0m\n\u001b[0m                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "94f420f1-8b45-4a6f-8873-e3b25b119498",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: '/home/users/vemundss/test'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3109118/1982407878.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mhome\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhome\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"test\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhome\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.9/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m         \u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;31m# Cannot rely on checking for EEXIST, since the operating system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: '/home/users/vemundss/test'"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "home = Path().home() / \"test\"\n",
    "os.makedirs(home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "e57e184f-fa9d-4ca6-b703-9821fb221c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 'a'), (2, 'c')}"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = {0 : 'a', 1 : 'b'}\n",
    "b = {1 : 'b', 2 : 'c'}\n",
    "a.items() ^ b.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "248a84f5-1e7f-40e5-a49a-0fa61ac04bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = {'a' : 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f7b453d8-a854-4d29-967c-0f2928442e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "for m in t.values():\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f030820b-2108-43d6-bb67-58b6102b0458",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on RNN in module torch.nn.modules.rnn object:\n",
      "\n",
      "class RNN(RNNBase)\n",
      " |  RNN(*args, **kwargs)\n",
      " |  \n",
      " |  Applies a multi-layer Elman RNN with :math:`\\tanh` or :math:`\\text{ReLU}` non-linearity to an\n",
      " |  input sequence.\n",
      " |  \n",
      " |  \n",
      " |  For each element in the input sequence, each layer computes the following\n",
      " |  function:\n",
      " |  \n",
      " |  .. math::\n",
      " |      h_t = \\tanh(W_{ih} x_t + b_{ih} + W_{hh} h_{(t-1)} + b_{hh})\n",
      " |  \n",
      " |  where :math:`h_t` is the hidden state at time `t`, :math:`x_t` is\n",
      " |  the input at time `t`, and :math:`h_{(t-1)}` is the hidden state of the\n",
      " |  previous layer at time `t-1` or the initial hidden state at time `0`.\n",
      " |  If :attr:`nonlinearity` is ``'relu'``, then :math:`\\text{ReLU}` is used instead of :math:`\\tanh`.\n",
      " |  \n",
      " |  Args:\n",
      " |      input_size: The number of expected features in the input `x`\n",
      " |      hidden_size: The number of features in the hidden state `h`\n",
      " |      num_layers: Number of recurrent layers. E.g., setting ``num_layers=2``\n",
      " |          would mean stacking two RNNs together to form a `stacked RNN`,\n",
      " |          with the second RNN taking in outputs of the first RNN and\n",
      " |          computing the final results. Default: 1\n",
      " |      nonlinearity: The non-linearity to use. Can be either ``'tanh'`` or ``'relu'``. Default: ``'tanh'``\n",
      " |      bias: If ``False``, then the layer does not use bias weights `b_ih` and `b_hh`.\n",
      " |          Default: ``True``\n",
      " |      batch_first: If ``True``, then the input and output tensors are provided\n",
      " |          as `(batch, seq, feature)` instead of `(seq, batch, feature)`.\n",
      " |          Note that this does not apply to hidden or cell states. See the\n",
      " |          Inputs/Outputs sections below for details.  Default: ``False``\n",
      " |      dropout: If non-zero, introduces a `Dropout` layer on the outputs of each\n",
      " |          RNN layer except the last layer, with dropout probability equal to\n",
      " |          :attr:`dropout`. Default: 0\n",
      " |      bidirectional: If ``True``, becomes a bidirectional RNN. Default: ``False``\n",
      " |  \n",
      " |  Inputs: input, h_0\n",
      " |      * **input**: tensor of shape :math:`(L, N, H_{in})` when ``batch_first=False`` or\n",
      " |        :math:`(N, L, H_{in})` when ``batch_first=True`` containing the features of\n",
      " |        the input sequence.  The input can also be a packed variable length sequence.\n",
      " |        See :func:`torch.nn.utils.rnn.pack_padded_sequence` or\n",
      " |        :func:`torch.nn.utils.rnn.pack_sequence` for details.\n",
      " |      * **h_0**: tensor of shape :math:`(D * \\text{num\\_layers}, N, H_{out})` containing the initial hidden\n",
      " |        state for each element in the batch. Defaults to zeros if not provided.\n",
      " |  \n",
      " |      where:\n",
      " |  \n",
      " |      .. math::\n",
      " |          \\begin{aligned}\n",
      " |              N ={} & \\text{batch size} \\\\\n",
      " |              L ={} & \\text{sequence length} \\\\\n",
      " |              D ={} & 2 \\text{ if bidirectional=True otherwise } 1 \\\\\n",
      " |              H_{in} ={} & \\text{input\\_size} \\\\\n",
      " |              H_{out} ={} & \\text{hidden\\_size}\n",
      " |          \\end{aligned}\n",
      " |  \n",
      " |  Outputs: output, h_n\n",
      " |      * **output**: tensor of shape :math:`(L, N, D * H_{out})` when ``batch_first=False`` or\n",
      " |        :math:`(N, L, D * H_{out})` when ``batch_first=True`` containing the output features\n",
      " |        `(h_t)` from the last layer of the RNN, for each `t`. If a\n",
      " |        :class:`torch.nn.utils.rnn.PackedSequence` has been given as the input, the output\n",
      " |        will also be a packed sequence.\n",
      " |      * **h_n**: tensor of shape :math:`(D * \\text{num\\_layers}, N, H_{out})` containing the final hidden state\n",
      " |        for each element in the batch.\n",
      " |  \n",
      " |  Attributes:\n",
      " |      weight_ih_l[k]: the learnable input-hidden weights of the k-th layer,\n",
      " |          of shape `(hidden_size, input_size)` for `k = 0`. Otherwise, the shape is\n",
      " |          `(hidden_size, num_directions * hidden_size)`\n",
      " |      weight_hh_l[k]: the learnable hidden-hidden weights of the k-th layer,\n",
      " |          of shape `(hidden_size, hidden_size)`\n",
      " |      bias_ih_l[k]: the learnable input-hidden bias of the k-th layer,\n",
      " |          of shape `(hidden_size)`\n",
      " |      bias_hh_l[k]: the learnable hidden-hidden bias of the k-th layer,\n",
      " |          of shape `(hidden_size)`\n",
      " |  \n",
      " |  .. note::\n",
      " |      All the weights and biases are initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`\n",
      " |      where :math:`k = \\frac{1}{\\text{hidden\\_size}}`\n",
      " |  \n",
      " |  .. note::\n",
      " |      For bidirectional RNNs, forward and backward are directions 0 and 1 respectively.\n",
      " |      Example of splitting the output layers when ``batch_first=False``:\n",
      " |      ``output.view(seq_len, batch, num_directions, hidden_size)``.\n",
      " |  \n",
      " |  .. include:: ../cudnn_rnn_determinism.rst\n",
      " |  \n",
      " |  .. include:: ../cudnn_persistent_rnn.rst\n",
      " |  \n",
      " |  Examples::\n",
      " |  \n",
      " |      >>> rnn = nn.RNN(10, 20, 2)\n",
      " |      >>> input = torch.randn(5, 3, 10)\n",
      " |      >>> h0 = torch.randn(2, 3, 20)\n",
      " |      >>> output, hn = rnn(input, h0)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      RNN\n",
      " |      RNNBase\n",
      " |      torch.nn.modules.module.Module\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *args, **kwargs)\n",
      " |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from RNNBase:\n",
      " |  \n",
      " |  __setattr__(self, attr, value)\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, d)\n",
      " |  \n",
      " |  check_forward_args(self, input: torch.Tensor, hidden: torch.Tensor, batch_sizes: Optional[torch.Tensor])\n",
      " |  \n",
      " |  check_hidden_size(self, hx: torch.Tensor, expected_hidden_size: Tuple[int, int, int], msg: str = 'Expected hidden size {}, got {}') -> None\n",
      " |  \n",
      " |  check_input(self, input: torch.Tensor, batch_sizes: Optional[torch.Tensor]) -> None\n",
      " |  \n",
      " |  extra_repr(self) -> str\n",
      " |      Set the extra representation of the module\n",
      " |      \n",
      " |      To print customized extra information, you should re-implement\n",
      " |      this method in your own modules. Both single-line and multi-line\n",
      " |      strings are acceptable.\n",
      " |  \n",
      " |  flatten_parameters(self) -> None\n",
      " |      Resets parameter data pointer so that they can use faster code paths.\n",
      " |      \n",
      " |      Right now, this works only if the module is on the GPU and cuDNN is enabled.\n",
      " |      Otherwise, it's a no-op.\n",
      " |  \n",
      " |  forward(self, input: Union[torch.Tensor, torch.nn.utils.rnn.PackedSequence], hx: Optional[torch.Tensor] = None) -> Tuple[Union[torch.Tensor, torch.nn.utils.rnn.PackedSequence], torch.Tensor]\n",
      " |      Defines the computation performed at every call.\n",
      " |      \n",
      " |      Should be overridden by all subclasses.\n",
      " |      \n",
      " |      .. note::\n",
      " |          Although the recipe for forward pass needs to be defined within\n",
      " |          this function, one should call the :class:`Module` instance afterwards\n",
      " |          instead of this since the former takes care of running the\n",
      " |          registered hooks while the latter silently ignores them.\n",
      " |  \n",
      " |  get_expected_hidden_size(self, input: torch.Tensor, batch_sizes: Optional[torch.Tensor]) -> Tuple[int, int, int]\n",
      " |  \n",
      " |  permute_hidden(self, hx: torch.Tensor, permutation: Optional[torch.Tensor])\n",
      " |  \n",
      " |  reset_parameters(self) -> None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from RNNBase:\n",
      " |  \n",
      " |  all_weights\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from RNNBase:\n",
      " |  \n",
      " |  __annotations__ = {'batch_first': <class 'bool'>, 'bias': <class 'bool...\n",
      " |  \n",
      " |  __constants__ = ['mode', 'input_size', 'hidden_size', 'num_layers', 'b...\n",
      " |  \n",
      " |  __jit_unused_properties__ = ['all_weights']\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __call__ = _call_impl(self, *input, **kwargs)\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __dir__(self)\n",
      " |      Default dir() implementation.\n",
      " |  \n",
      " |  __getattr__(self, name: str) -> Union[torch.Tensor, ForwardRef('Module')]\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  add_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
      " |      Adds a child module to the current module.\n",
      " |      \n",
      " |      The module can be accessed as an attribute using the given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the child module. The child module can be\n",
      " |              accessed from this module using the given name\n",
      " |          module (Module): child module to be added to the module.\n",
      " |  \n",
      " |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      " |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      " |      as well as self. Typical use includes initializing the parameters of a model\n",
      " |      (see also :ref:`nn-init-doc`).\n",
      " |      \n",
      " |      Args:\n",
      " |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> @torch.no_grad()\n",
      " |          >>> def init_weights(m):\n",
      " |          >>>     print(m)\n",
      " |          >>>     if type(m) == nn.Linear:\n",
      " |          >>>         m.weight.fill_(1.0)\n",
      " |          >>>         print(m.weight)\n",
      " |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      " |          >>> net.apply(init_weights)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 1.,  1.],\n",
      " |                  [ 1.,  1.]])\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 1.,  1.],\n",
      " |                  [ 1.,  1.]])\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |  \n",
      " |  bfloat16(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      " |      Returns an iterator over module buffers.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          torch.Tensor: module buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for buf in model.buffers():\n",
      " |          >>>     print(type(buf), buf.size())\n",
      " |          <class 'torch.Tensor'> (20L,)\n",
      " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  children(self) -> Iterator[ForwardRef('Module')]\n",
      " |      Returns an iterator over immediate children modules.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a child module\n",
      " |  \n",
      " |  cpu(self: ~T) -> ~T\n",
      " |      Moves all model parameters and buffers to the CPU.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Moves all model parameters and buffers to the GPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on GPU while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  double(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  eval(self: ~T) -> ~T\n",
      " |      Sets the module in evaluation mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      " |      \n",
      " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      " |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  float(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  get_buffer(self, target: str) -> 'Tensor'\n",
      " |      Returns the buffer given by ``target`` if it exists,\n",
      " |      otherwise throws an error.\n",
      " |      \n",
      " |      See the docstring for ``get_submodule`` for a more detailed\n",
      " |      explanation of this method's functionality as well as how to\n",
      " |      correctly specify ``target``.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the buffer\n",
      " |              to look for. (See ``get_submodule`` for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.Tensor: The buffer referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not a\n",
      " |              buffer\n",
      " |  \n",
      " |  get_extra_state(self) -> Any\n",
      " |      Returns any extra state to include in the module's state_dict.\n",
      " |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      " |      if you need to store extra state. This function is called when building the\n",
      " |      module's `state_dict()`.\n",
      " |      \n",
      " |      Note that extra state should be pickleable to ensure working serialization\n",
      " |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      " |      for serializing Tensors; other objects may break backwards compatibility if\n",
      " |      their serialized pickled form changes.\n",
      " |      \n",
      " |      Returns:\n",
      " |          object: Any extra state to store in the module's state_dict\n",
      " |  \n",
      " |  get_parameter(self, target: str) -> 'Parameter'\n",
      " |      Returns the parameter given by ``target`` if it exists,\n",
      " |      otherwise throws an error.\n",
      " |      \n",
      " |      See the docstring for ``get_submodule`` for a more detailed\n",
      " |      explanation of this method's functionality as well as how to\n",
      " |      correctly specify ``target``.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the Parameter\n",
      " |              to look for. (See ``get_submodule`` for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not an\n",
      " |              ``nn.Parameter``\n",
      " |  \n",
      " |  get_submodule(self, target: str) -> 'Module'\n",
      " |      Returns the submodule given by ``target`` if it exists,\n",
      " |      otherwise throws an error.\n",
      " |      \n",
      " |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      " |      looks like this:\n",
      " |      \n",
      " |      .. code-block::text\n",
      " |      \n",
      " |          A(\n",
      " |              (net_b): Module(\n",
      " |                  (net_c): Module(\n",
      " |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      " |                  )\n",
      " |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      " |              )\n",
      " |          )\n",
      " |      \n",
      " |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      " |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      " |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      " |      \n",
      " |      To check whether or not we have the ``linear`` submodule, we\n",
      " |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      " |      we have the ``conv`` submodule, we would call\n",
      " |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      " |      \n",
      " |      The runtime of ``get_submodule`` is bounded by the degree\n",
      " |      of module nesting in ``target``. A query against\n",
      " |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      " |      the number of transitive modules. So, for a simple check to see\n",
      " |      if some submodule exists, ``get_submodule`` should always be\n",
      " |      used.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the submodule\n",
      " |              to look for. (See above example for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.nn.Module: The submodule referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not an\n",
      " |              ``nn.Module``\n",
      " |  \n",
      " |  half(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]', strict: bool = True)\n",
      " |      Copies parameters and buffers from :attr:`state_dict` into\n",
      " |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      " |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      " |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      " |      \n",
      " |      Args:\n",
      " |          state_dict (dict): a dict containing parameters and\n",
      " |              persistent buffers.\n",
      " |          strict (bool, optional): whether to strictly enforce that the keys\n",
      " |              in :attr:`state_dict` match the keys returned by this module's\n",
      " |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      " |      \n",
      " |      Returns:\n",
      " |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      " |              * **missing_keys** is a list of str containing the missing keys\n",
      " |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      " |      \n",
      " |      Note:\n",
      " |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      " |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      " |          ``RuntimeError``.\n",
      " |  \n",
      " |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      " |      Returns an iterator over all modules in the network.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a module in the network\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.modules()):\n",
      " |                  print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      " |  \n",
      " |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      " |      Returns an iterator over module buffers, yielding both the\n",
      " |      name of the buffer as well as the buffer itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all buffer names.\n",
      " |          recurse (bool): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, buf in self.named_buffers():\n",
      " |          >>>    if name in ['running_var']:\n",
      " |          >>>        print(buf.size())\n",
      " |  \n",
      " |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      " |      Returns an iterator over immediate children modules, yielding both\n",
      " |      the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple containing a name and child module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, module in model.named_children():\n",
      " |          >>>     if name in ['conv4', 'conv5']:\n",
      " |          >>>         print(module)\n",
      " |  \n",
      " |  named_modules(self, memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      " |      Returns an iterator over all modules in the network, yielding\n",
      " |      both the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          memo: a memo to store the set of modules already added to the result\n",
      " |          prefix: a prefix that will be added to the name of the module\n",
      " |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      " |          or not\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple of name and module\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.named_modules()):\n",
      " |                  print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> ('', Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          ))\n",
      " |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      " |  \n",
      " |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      " |      Returns an iterator over module parameters, yielding both the\n",
      " |      name of the parameter as well as the parameter itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all parameter names.\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Parameter): Tuple containing the name and parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, param in self.named_parameters():\n",
      " |          >>>    if name in ['bias']:\n",
      " |          >>>        print(param.size())\n",
      " |  \n",
      " |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      " |      Returns an iterator over module parameters.\n",
      " |      \n",
      " |      This is typically passed to an optimizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Parameter: module parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for param in model.parameters():\n",
      " |          >>>     print(type(param), param.size())\n",
      " |          <class 'torch.Tensor'> (20L,)\n",
      " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Optional[torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      " |      the behavior of this function will change in future versions.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_buffer(self, name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None\n",
      " |      Adds a buffer to the module.\n",
      " |      \n",
      " |      This is typically used to register a buffer that should not to be\n",
      " |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      " |      is not a parameter, but is part of the module's state. Buffers, by\n",
      " |      default, are persistent and will be saved alongside parameters. This\n",
      " |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      " |      only difference between a persistent buffer and a non-persistent buffer\n",
      " |      is that the latter will not be a part of this module's\n",
      " |      :attr:`state_dict`.\n",
      " |      \n",
      " |      Buffers can be accessed as attributes using given names.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the buffer. The buffer can be accessed\n",
      " |              from this module using the given name\n",
      " |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      " |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      " |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      " |          persistent (bool): whether the buffer is part of this module's\n",
      " |              :attr:`state_dict`.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      " |  \n",
      " |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a forward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time after :func:`forward` has computed an output.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input, output) -> None or modified output\n",
      " |      \n",
      " |      The input contains only the positional arguments given to the module.\n",
      " |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      " |      The hook can modify the output. It can modify the input inplace but\n",
      " |      it will not have effect on forward since this is called after\n",
      " |      :func:`forward` is called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a forward pre-hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time before :func:`forward` is invoked.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input) -> None or modified input\n",
      " |      \n",
      " |      The input contains only the positional arguments given to the module.\n",
      " |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      " |      The hook can modify the input. User can either return a tuple or a\n",
      " |      single modified value in the hook. We will wrap the value into a tuple\n",
      " |      if a single value is returned(unless that value is already a tuple).\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Optional[torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients with respect to module\n",
      " |      inputs are computed. The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      " |      \n",
      " |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      " |      with respect to the inputs and outputs respectively. The hook should\n",
      " |      not modify its arguments, but it can optionally return a new gradient with\n",
      " |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      " |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      " |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      " |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      " |      arguments.\n",
      " |      \n",
      " |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      " |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      " |      of each Tensor returned by the Module's forward function.\n",
      " |      \n",
      " |      .. warning ::\n",
      " |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      " |          will raise an error.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_parameter(self, name: str, param: Optional[torch.nn.parameter.Parameter]) -> None\n",
      " |      Adds a parameter to the module.\n",
      " |      \n",
      " |      The parameter can be accessed as an attribute using given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the parameter. The parameter can be accessed\n",
      " |              from this module using the given name\n",
      " |          param (Parameter or None): parameter to be added to the module. If\n",
      " |              ``None``, then operations that run on parameters, such as :attr:`cuda`,\n",
      " |              are ignored. If ``None``, the parameter is **not** included in the\n",
      " |              module's :attr:`state_dict`.\n",
      " |  \n",
      " |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      " |      Change if autograd should record operations on parameters in this\n",
      " |      module.\n",
      " |      \n",
      " |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      " |      in-place.\n",
      " |      \n",
      " |      This method is helpful for freezing part of the module for finetuning\n",
      " |      or training parts of a model individually (e.g., GAN training).\n",
      " |      \n",
      " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      " |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      " |      \n",
      " |      Args:\n",
      " |          requires_grad (bool): whether autograd should record operations on\n",
      " |                                parameters in this module. Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  set_extra_state(self, state: Any)\n",
      " |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      " |      found within the `state_dict`. Implement this function and a corresponding\n",
      " |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      " |      `state_dict`.\n",
      " |      \n",
      " |      Args:\n",
      " |          state (dict): Extra state from the `state_dict`\n",
      " |  \n",
      " |  share_memory(self: ~T) -> ~T\n",
      " |      See :meth:`torch.Tensor.share_memory_`\n",
      " |  \n",
      " |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      " |      Returns a dictionary containing a whole state of the module.\n",
      " |      \n",
      " |      Both parameters and persistent buffers (e.g. running averages) are\n",
      " |      included. Keys are corresponding parameter and buffer names.\n",
      " |      Parameters and buffers set to ``None`` are not included.\n",
      " |      \n",
      " |      Returns:\n",
      " |          dict:\n",
      " |              a dictionary containing a whole state of the module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> module.state_dict().keys()\n",
      " |          ['bias', 'weight']\n",
      " |  \n",
      " |  to(self, *args, **kwargs)\n",
      " |      Moves and/or casts the parameters and buffers.\n",
      " |      \n",
      " |      This can be called as\n",
      " |      \n",
      " |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      .. function:: to(dtype, non_blocking=False)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      .. function:: to(tensor, non_blocking=False)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      .. function:: to(memory_format=torch.channels_last)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      " |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      " |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      " |      (if given). The integral parameters and buffers will be moved\n",
      " |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      " |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      " |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      " |      pinned memory to CUDA devices.\n",
      " |      \n",
      " |      See below for examples.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): the desired device of the parameters\n",
      " |              and buffers in this module\n",
      " |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      " |              the parameters and buffers in this module\n",
      " |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      " |              dtype and device for all parameters and buffers in this module\n",
      " |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      " |              format for 4D parameters and buffers in this module (keyword\n",
      " |              only argument)\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          >>> linear = nn.Linear(2, 2)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]])\n",
      " |          >>> linear.to(torch.double)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      " |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      " |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      " |          >>> cpu = torch.device(\"cpu\")\n",
      " |          >>> linear.to(cpu)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      " |      \n",
      " |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      " |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      " |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      " |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      " |                  [0.6122+0.j, 0.1150+0.j],\n",
      " |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      " |  \n",
      " |  to_empty(self: ~T, *, device: Union[str, torch.device]) -> ~T\n",
      " |      Moves the parameters and buffers to the specified device without copying storage.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): The desired device of the parameters\n",
      " |              and buffers in this module.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  train(self: ~T, mode: bool = True) -> ~T\n",
      " |      Sets the module in training mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      Args:\n",
      " |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      " |                       mode (``False``). Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      " |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          dst_type (type or string): the desired type\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Moves all model parameters and buffers to the XPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on XPU while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  zero_grad(self, set_to_none: bool = False) -> None\n",
      " |      Sets gradients of all model parameters to zero. See similar function\n",
      " |      under :class:`torch.optim.Optimizer` for more context.\n",
      " |      \n",
      " |      Args:\n",
      " |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      " |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  T_destination = ~T_destination\n",
      " |  \n",
      " |  dump_patches = False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa52361-8121-482f-9689-990c35679a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.prune_mask = [2,3,4,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "7ff4a629-2f2c-48de-946c-235c24455b11",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/users/vemundss/test'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3109118/455204409.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfnames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mfilenames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhome\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_3109118/455204409.py\u001b[0m in \u001b[0;36mfilenames\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m\"\"\"Return (sorted) names of all files in given path\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mfnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mfnames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfnames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/users/vemundss/test'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "def filenames(path):\n",
    "    \"\"\"Return (sorted) names of all files in given path\"\"\"\n",
    "    fnames = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n",
    "    fnames.sort()\n",
    "    return fnames\n",
    "\n",
    "filenames(Path().home() / \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "959a029f-a384-4923-8234-126ac7fd4113",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros(N, L, Hin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2bb754e6-f56c-4232-a8f1-571f81b7c5f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 5])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h0 = torch.ones(n_rnns, N, Hout)\n",
    "h0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a2eb3124-585f-4df6-9b2c-c7909f4d39b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 2., 1., 1., 1.]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn(x,h0)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "81fa29ea-e67d-4b0b-bfa4-44041b62c96f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0.]],\n",
       "\n",
       "        [[0., 0.]],\n",
       "\n",
       "        [[0., 0.]]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "beed124e-27c7-43c9-ac3e-398ab41632c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn(x)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fba1329-dd5c-493b-93a4-a212ca36b937",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3e38435a-8e7e-4d39-a9c6-3791cdf12b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,) 2 {'c': 4}\n"
     ]
    }
   ],
   "source": [
    "def f(x=2, *args, **kwargs):\n",
    "    print(args,x,kwargs)\n",
    "    \n",
    "f(2,3,c=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5a171dd7-c18c-4bfb-874a-fddfd45d5ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 2]) torch.Size([1, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(N, L, Hin)\n",
    "h0 = torch.randn(n_rnns, N, Hout)\n",
    "print(x.shape, h0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5ef8d562-c816-434b-9e4c-f7ce783462aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = rnn(x,h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8bfcafea-6d0c-4095-aebc-8e11f30b3c2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 4, 5]), torch.Size([1, 3, 5]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[0].shape, test[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d153c69f-0b7f-4a4e-9443-0cb41fc4faf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d69a6de-d79f-48c8-8260-cc6579c8a622",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1,t2 = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c884c7d2-592f-491a-ab83-871b2b6ed50c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 4, 5]), torch.Size([1, 3, 5]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.shape, t2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "770dd94c-d167-4dd8-9022-9cc2485ee01e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[True, True, True, True, True],\n",
       "         [True, True, True, True, True],\n",
       "         [True, True, True, True, True]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1[:,3] == t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc8f66be-a9ae-436f-8d05-6db42bb0b731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(np.array(10)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aeafed49-70d0-4623-993c-24bd159e787c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\") if \"../src\" not in sys.path else None # avoid adding multiple relave paths to sys.path\n",
    "from methods import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3471026d-e34b-46b4-8385-05d67fceca94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<methods.Dataset at 0x7fab441c1f70>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2fee52e-a008-4583-a9c9-028245a51956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1])\n",
      "tensor([[0., 1.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.3133)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TORCH CROSS ENTROPY DOES NOT SUPPORT \"MULTI TARGET\" - i.e. true cross-entropy\n",
    "X, label = torch.ones((1,2)), torch.ones(1,dtype=torch.int64)\n",
    "#label[0,0] = 0\n",
    "print(label)\n",
    "X[0,0] = 0\n",
    "print(X)\n",
    "torch.nn.functional.cross_entropy(input=X, target=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17954c95-615a-411a-aea0-47c699085504",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.nn.Linear(2,3).weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "baa87c25-4be2-4b32-a8ef-cbd022ae4dbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "814ee401-2c78-4fef-b529-35744b1fe4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 2\n",
    "\n",
    "g = f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfff3f3-113a-4768-bbc4-c516ecfcd16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "g("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6bd975b-b90d-4c1a-9b41-c6261fa86a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "93d1a7fb-b66b-4ef8-b38f-1922d3848126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.7600,  0.7474], requires_grad=True), tensor([0.8973]))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b = torch.randn(2, requires_grad=True), torch.randn(1, requires_grad=False)\n",
    "a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3e70059d-870d-4710-ac3e-84572f62db85",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = torch.sum(a**2 * b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5290489a-9373-4a42-a330-4bd0eb97ff9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "external_grad = torch.tensor(1.)\n",
    "external_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "50f3be7d-b48e-4fef-9af6-733f2875cfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q.backward(gradient=external_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ce09f571-fd00-4a3e-852c-33c18bc503a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "external_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "cb75ff40-9b11-4454-895e-5c74ca5997f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.5978,  3.3204])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "76a9d390-a2f2-475b-945f-804e16a20d2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.7915,  1.6447], requires_grad=True)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a520b707-0e4f-4045-a24d-e854f111f36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0094])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ac08ef-2214-4b6b-9790-2e7cc7e27006",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "46df1872-6963-4866-a9aa-259dd846ea81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1.])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)\n",
    "Q = 3*a**3 - b**2\n",
    "external_grad = torch.tensor([1., 1.])\n",
    "Q.backward(gradient=external_grad)\n",
    "external_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1d5b589f-1d5c-46d4-abb7-090bd4c8dc0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(b) == torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "5521be00-d733-4fc7-8fad-5c5d5a30c7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((5,4,3))\n",
    "values, idxs = torch.topk(x, k=2, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "b603efec-48a9-44e2-8d65-a59b74b3569b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 4, 2]), torch.Size([5, 4, 2]))"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values.shape, idxs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "0fcfa6ae-86b2-43ac-8022-099b9b09d63a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 4, 2])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.gather(x, -1, idxs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "15e2c457-3b0a-4142-b759-eba8c89148d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "{'b': 3, 'c': 4} <class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "def f(a, **kwargs):\n",
    "    print(a)\n",
    "    print(kwargs, type(kwargs))\n",
    "\n",
    "d = {'a': 2, 'b': 3, 'c':4}\n",
    "f(**d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f6f650-2bec-4c64-ab67-f77a176dd406",
   "metadata": {},
   "outputs": [],
   "source": [
    "def "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "75171dca-7bab-4373-aa0d-51f1e852a007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5, 2]) torch.Size([10, 5])\n"
     ]
    }
   ],
   "source": [
    "X,Y = torch.randn((10,2)), torch.randn((5,2))\n",
    "mandists = torch.sum((X[:,None]-Y[None])**2, axis=-1)\n",
    "print((X[:,None]-Y[None]).shape, mandists.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f81ee4-1474-473d-9508-3052e0e27df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "[10,1,2] + [1,5,2] => [10,5,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b34ccfd4-1a85-4840-b013-839fcb067a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = torch.cdist(X,Y,compute_mode='use_mm_for_euclid_dist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "359c1aaf-4b84-470f-a4e6-95ac98c58ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.1127e+00, 9.3647e+00, 1.0746e+00, 4.4152e-01, 5.6463e+00],\n",
       "        [5.6894e+00, 8.7903e+00, 2.3298e+00, 3.7816e+00, 1.0156e+00],\n",
       "        [2.4815e+00, 6.7801e+00, 3.8200e-01, 1.0728e-02, 3.1811e+00],\n",
       "        [3.3088e+00, 7.7801e+00, 2.5535e+00, 1.2073e+00, 6.8445e+00],\n",
       "        [2.7053e-01, 2.0636e+00, 1.9724e+00, 1.4716e+00, 1.1450e+00],\n",
       "        [6.6821e-01, 3.3748e+00, 1.8501e+00, 8.6606e-01, 3.0146e+00],\n",
       "        [2.4013e+00, 8.3213e-01, 1.1653e+01, 9.4784e+00, 7.8536e+00],\n",
       "        [8.1488e-01, 3.6932e+00, 1.0105e+00, 4.7989e-01, 1.8623e+00],\n",
       "        [1.2363e+00, 3.5936e+00, 1.1402e+00, 1.2455e+00, 4.0108e-01],\n",
       "        [3.1170e-01, 2.2498e+00, 3.0116e+00, 1.7721e+00, 3.3362e+00]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mandists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "73b9f344-4b28-4f97-ae0d-8144722da479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.0280, 3.0602, 1.0366, 0.6645, 2.3762],\n",
       "        [2.3852, 2.9648, 1.5264, 1.9446, 1.0078],\n",
       "        [1.5753, 2.6039, 0.6181, 0.1036, 1.7836],\n",
       "        [1.8190, 2.7893, 1.5980, 1.0988, 2.6162],\n",
       "        [0.5201, 1.4365, 1.4044, 1.2131, 1.0700],\n",
       "        [0.8174, 1.8371, 1.3602, 0.9306, 1.7363],\n",
       "        [1.5496, 0.9122, 3.4137, 3.0787, 2.8024],\n",
       "        [0.9027, 1.9218, 1.0052, 0.6927, 1.3647],\n",
       "        [1.1119, 1.8957, 1.0678, 1.1160, 0.6333],\n",
       "        [0.5583, 1.4999, 1.7354, 1.3312, 1.8265]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1f9a550e-e185-4cc3-9574-f7ce531c36b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 3\n",
      "{'c': 4}\n"
     ]
    }
   ],
   "source": [
    "def g(a,b, **kwargs):\n",
    "    print(a,b)\n",
    "    print(kwargs)\n",
    "\n",
    "kwargs = {'a': 2, 'b':3, 'c': 4}\n",
    "g(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a09ee01-c1a9-40ff-8c3b-90a37509ae27",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = g"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
